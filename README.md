# Price Sense í¬ë¡¤ë§ ì‹œìŠ¤í…œ

Price Sense B2B SaaS ì„œë¹„ìŠ¤ì˜ Python ê¸°ë°˜ í¬ë¡¤ë§ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. í•œêµ­ ì£¼ìš” ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼ì—ì„œ ìƒí’ˆ ê°€ê²© ë° ì¬ê³  ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ì§€ì› í”Œë«í¼](#ì§€ì›-í”Œë«í¼)
- [ì„¤ì¹˜ ë° ì„¤ì •](#ì„¤ì¹˜-ë°-ì„¤ì •)
- [í™˜ê²½ ë³€ìˆ˜ ì„¤ì •](#í™˜ê²½-ë³€ìˆ˜-ì„¤ì •)
- [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
- [ì•„í‚¤í…ì²˜](#ì•„í‚¤í…ì²˜)
- [ê°œë°œ ê°€ì´ë“œ](#ê°œë°œ-ê°€ì´ë“œ)
- [ëª¨ë‹ˆí„°ë§](#ëª¨ë‹ˆí„°ë§)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ğŸ›’ ì§€ì› í”Œë«í¼

í˜„ì¬ ì§€ì›í•˜ëŠ” ì´ì»¤ë¨¸ìŠ¤ í”Œë«í¼:

- **ì¿ íŒ¡** (coupang.com) - Selenium ê¸°ë°˜
- **ë„¤ì´ë²„ì‡¼í•‘** (shopping.naver.com) - HTTP + Selenium í•˜ì´ë¸Œë¦¬ë“œ
- **ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´** (smartstore.naver.com) - React ì•± ì „ìš©

## ğŸ”§ ì„¤ì¹˜ ë° ì„¤ì •

### 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

- Python 3.11+
- Chrome/Chromium ë¸Œë¼ìš°ì €
- Redis ì„œë²„
- PostgreSQL ë°ì´í„°ë² ì´ìŠ¤

### 2. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
git clone <repository-url>
cd price-sense-crawling

# Python ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 3. Chrome ë“œë¼ì´ë²„ ì„¤ì¹˜

```bash
# macOS (Homebrew)
brew install chromedriver

# Ubuntu
sudo apt-get update
sudo apt-get install chromium-chromedriver

# ë˜ëŠ” ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
# https://chromedriver.chromium.org/
```

## âš™ï¸ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‹¤ìŒ ì„¤ì •ì„ ì¶”ê°€í•˜ì„¸ìš”:

```env
# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
DATABASE_URL=postgresql://username:password@localhost:5432/pricesense

# Redis ì—°ê²°
REDIS_URL=redis://localhost:6379/0

# í¬ë¡¤ëŸ¬ ì„¤ì •
LOG_LEVEL=INFO
CHROME_DRIVER_PATH=/usr/local/bin/chromedriver

# Redis í ì´ë¦„ (ì„ íƒì‚¬í•­)
CRAWL_QUEUE_NAME=price_crawl_tasks
RESULT_QUEUE_NAME=price_crawl_results
DEAD_LETTER_QUEUE=price_crawl_failed

# ì„±ëŠ¥ ì„¤ì •
MAX_RETRIES=3
REQUEST_DELAY=2.0
```

### í™˜ê²½ ë³€ìˆ˜ ì„¤ëª…

| ë³€ìˆ˜ëª… | í•„ìˆ˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|--------|------|--------|------|
| `DATABASE_URL` | âœ… | - | PostgreSQL ì—°ê²° URL |
| `REDIS_URL` | âœ… | - | Redis ì—°ê²° URL |
| `LOG_LEVEL` | âŒ | INFO | ë¡œê·¸ ë ˆë²¨ (DEBUG/INFO/WARNING/ERROR) |
| `CHROME_DRIVER_PATH` | âŒ | ìë™ íƒì§€ | ChromeDriver ì‹¤í–‰ íŒŒì¼ ê²½ë¡œ |
| `MAX_RETRIES` | âŒ | 3 | ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ |
| `REQUEST_DELAY` | âŒ | 2.0 | ìš”ì²­ ê°„ ê¸°ë³¸ ì§€ì—° ì‹œê°„(ì´ˆ) |

## ğŸš€ ì‚¬ìš©ë²•

### 1. ê¸°ë³¸ ì‹¤í–‰

```bash
# ë‹¨ì¼ ì›Œì»¤ ì‹¤í–‰
python -m crawlers.worker

# ë©€í‹° ì›Œì»¤ ì‹¤í–‰ (4ê°œ í”„ë¡œì„¸ìŠ¤)
python -m crawlers.worker --workers 4

# ì»¤ìŠ¤í…€ ì›Œì»¤ ID ì„¤ì •
python -m crawlers.worker --workers 2 --worker-prefix "prod-worker"
```

### 2. ê°œë³„ í”Œë«í¼ í…ŒìŠ¤íŠ¸

ê°œë³„ í¬ë¡¤ëŸ¬ë¥¼ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´:

```bash
# ì¿ íŒ¡ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
python -m crawlers.worker --test coupang \
  --url "https://www.coupang.com/vp/products/6339984726" \
  --product-id "test-001"

# ë„¤ì´ë²„ì‡¼í•‘ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
python -m crawlers.worker --test naver_shopping \
  --url "https://shopping.naver.com/catalog/products/12345" \
  --product-id "test-002"

# ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸
python -m crawlers.worker --test smartstore \
  --url "https://smartstore.naver.com/store/products/12345" \
  --product-id "test-003"
```

### 3. ë¡œê·¸ ë ˆë²¨ ì¡°ì •

```bash
# ë””ë²„ê·¸ ëª¨ë“œ
python -m crawlers.worker --log-level DEBUG

# ì—ëŸ¬ë§Œ ì¶œë ¥
python -m crawlers.worker --log-level ERROR --workers 4
```

### 4. Redis íì— ì‘ì—… ì¶”ê°€

Python ì½”ë“œë¡œ í¬ë¡¤ë§ ì‘ì—…ì„ íì— ì¶”ê°€:

```python
from storage.redis_client import task_queue

# ì‘ì—… ë°ì´í„°
task_data = {
    "task_id": "550e8400-e29b-41d4-a716-446655440000",
    "product_id": "12345",
    "url": "https://www.coupang.com/vp/products/6339984726",
    "platform": "coupang",
    "user_id": "user123"
}

# íì— ì¶”ê°€ (ì¼ë°˜ ìš°ì„ ìˆœìœ„)
success = task_queue.push_task(task_data, priority="normal")

# ë†’ì€ ìš°ì„ ìˆœìœ„ë¡œ ì¶”ê°€
success = task_queue.push_task(task_data, priority="high")
```

### 5. í ìƒíƒœ í™•ì¸

```python
from storage.redis_client import task_queue

# í í†µê³„ ì¡°íšŒ
stats = task_queue.get_queue_stats()
print(stats)
# ì¶œë ¥: {'crawl_high': 5, 'crawl_normal': 23, 'result': 1, 'dead_letter': 0}
```

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### ì‹œìŠ¤í…œ êµ¬ì„±ìš”ì†Œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   NestJS API    â”‚    â”‚  Redis Queues   â”‚    â”‚ Python Crawler  â”‚
â”‚     Server      â”‚â”€â”€â”€â–¶â”‚                 â”‚â”€â”€â”€â–¶â”‚     Workers     â”‚
â”‚                 â”‚    â”‚ â€¢ Tasks         â”‚    â”‚                 â”‚
â”‚ â€¢ ìŠ¤ì¼€ì¤„ë§       â”‚    â”‚ â€¢ Results       â”‚    â”‚ â€¢ í¬ë¡¤ë§ ì‹¤í–‰    â”‚
â”‚ â€¢ API ì œê³µ      â”‚    â”‚ â€¢ Failed        â”‚    â”‚ â€¢ ë°ì´í„° ê²€ì¦    â”‚
â”‚ â€¢ ì•Œë¦¼ ë°œì†¡     â”‚    â”‚                 â”‚    â”‚ â€¢ DB ì €ì¥       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â””â”€â”€â”€â”€â”€â–¶â”‚   PostgreSQL    â”‚
                                       â”‚    Database     â”‚
                                       â”‚                 â”‚
                                       â”‚ â€¢ ìƒí’ˆ ì •ë³´      â”‚
                                       â”‚ â€¢ ê°€ê²©/ì¬ê³  ì´ë ¥ â”‚
                                       â”‚ â€¢ í¬ë¡¤ë§ ë¡œê·¸    â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ íë¦„

```
Redis í â”€â”€â”
          â”œâ”€â–¶ Queue Handler â”€â”€â–¶ Platform Crawler â”€â”€â–¶ Data Validator â”€â”€â–¶ Database
          â”‚                                                             â”‚
          â”‚                  â”Œâ”€ Anti-Detection â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Results Queue
```

### ë°ì´í„° í”Œë¡œìš°

1. **NestJS ì„œë²„**ê°€ í¬ë¡¤ë§ ì‘ì—…ì„ Redis íì— ì¶”ê°€
2. **Python ì›Œì»¤**ê°€ íì—ì„œ ì‘ì—…ì„ ê°€ì ¸ì˜´
3. **í”Œë«í¼ë³„ í¬ë¡¤ëŸ¬**ê°€ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
4. **ë°ì´í„° ê²€ì¦**ìœ¼ë¡œ í’ˆì§ˆ í™•ì¸
5. **PostgreSQL**ì— ê°€ê²©/ì¬ê³  ì´ë ¥ ì €ì¥
6. **ê²°ê³¼**ë¥¼ Redis ê²°ê³¼ íë¡œ ì „ì†¡

## ğŸ› ï¸ ê°œë°œ ê°€ì´ë“œ

### ìƒˆë¡œìš´ í”Œë«í¼ ì¶”ê°€

1. `crawlers/platforms/` ë””ë ‰í† ë¦¬ì— ìƒˆ í¬ë¡¤ëŸ¬ íŒŒì¼ ìƒì„±
2. `BaseCrawler` í´ë˜ìŠ¤ ìƒì†
3. í•„ìˆ˜ ë©”ì„œë“œ êµ¬í˜„:

```python
from crawlers.core.base_crawler import BaseCrawler, CrawlResult
from models.base import PlatformType

class NewPlatformCrawler(BaseCrawler):
    def __init__(self, **kwargs):
        super().__init__(platform=PlatformType.NEW_PLATFORM, **kwargs)
    
    def _is_platform_url(self, url: str) -> bool:
        # URL ê²€ì¦ ë¡œì§
        return 'newplatform.com' in url.lower()
    
    def get_platform_selectors(self) -> Dict[str, str]:
        # CSS ì…€ë ‰í„° ì •ì˜
        return {
            'product_name': ['.product-title'],
            'current_price': ['.price'],
            # ...
        }
    
    async def extract_product_data(self, product_id: str, url: str) -> CrawlResult:
        # ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§
        pass
```

4. `queue_handler.py`ì— ìƒˆ í¬ë¡¤ëŸ¬ ë“±ë¡:

```python
self.crawler_classes[PlatformType.NEW_PLATFORM] = NewPlatformCrawler
```

### ì»¤ìŠ¤í…€ ê²€ì¦ ê·œì¹™ ì¶”ê°€

`utils/validation.py`ì—ì„œ ê²€ì¦ ê·œì¹™ì„ ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ê°€:

```python
class CustomValidator:
    def validate_custom_field(self, value: Any) -> ValidationResult:
        result = ValidationResult(True, 1.0, [], [])
        
        # ì»¤ìŠ¤í…€ ê²€ì¦ ë¡œì§
        if not self.is_valid_value(value):
            result.add_error("Custom validation failed")
            result.score = 0.0
        
        return result
```

### ì„¤ì • ë³€ê²½

`config/settings.py`ì—ì„œ ì‹œìŠ¤í…œ ì„¤ì •ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
class CrawlerSettings(BaseSettings):
    max_retries: int = 3
    request_delay: float = 2.0
    timeout: int = 30
    
    # í”Œë«í¼ë³„ ì§€ì—°ì‹œê°„ ì»¤ìŠ¤í„°ë§ˆì´ì§•
    platform_delays: Dict[str, float] = {
        "coupang": 3.0,
        "naver_shopping": 2.0,
        # ...
    }
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
tail -f logs/crawler.log

# ì—ëŸ¬ ë¡œê·¸ë§Œ í™•ì¸
grep "ERROR" logs/crawler.log

# íŠ¹ì • í”Œë«í¼ ë¡œê·¸ í•„í„°ë§
grep "coupang" logs/crawler.log
```

### Redis í ëª¨ë‹ˆí„°ë§

```python
from storage.redis_client import task_queue

# ì£¼ê¸°ì ìœ¼ë¡œ í ìƒíƒœ í™•ì¸
import time
while True:
    stats = task_queue.get_queue_stats()
    print(f"Queue status: {stats}")
    time.sleep(60)
```

### ì„±ëŠ¥ ë©”íŠ¸ë¦­

ì›Œì»¤ ì‹¤í–‰ ì¤‘ ì„±ëŠ¥ í†µê³„ê°€ ìë™ìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤:

```
=== Final Statistics ===
Runtime: 3600.0 seconds
Total tasks processed: 1250
Successful tasks: 1187
Failed tasks: 63
Success rate: 95.0%
Average time per task: 2.9 seconds
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. ChromeDriver ì˜¤ë¥˜

```
selenium.common.exceptions.WebDriverException: 'chromedriver' executable needs to be in PATH
```

**í•´ê²°ë°©ë²•:**
```bash
# ChromeDriver ì„¤ì¹˜ í™•ì¸
chromedriver --version

# PATHì— ì¶”ê°€í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export CHROME_DRIVER_PATH=/usr/local/bin/chromedriver
```

#### 2. Redis ì—°ê²° ì‹¤íŒ¨

```
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.
```

**í•´ê²°ë°©ë²•:**
```bash
# Redis ì„œë²„ ì‹œì‘
redis-server

# ë˜ëŠ” Dockerë¡œ ì‹¤í–‰
docker run -d -p 6379:6379 redis:alpine
```

#### 3. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜

```
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server
```

**í•´ê²°ë°©ë²•:**
```bash
# PostgreSQL ì„œë²„ ìƒíƒœ í™•ì¸
pg_ctl status

# ì—°ê²° URL í™•ì¸
echo $DATABASE_URL
```

#### 4. ë©”ëª¨ë¦¬ ë¶€ì¡±

ì›Œì»¤ê°€ ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì¢…ë£Œë˜ëŠ” ê²½ìš°:

```bash
# ì›Œì»¤ ìˆ˜ ì¤„ì´ê¸°
python -m crawlers.worker --workers 2

# ë˜ëŠ” ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì¦ì„¤
```

### ë””ë²„ê¹… ëª¨ë“œ

ìƒì„¸í•œ ë””ë²„ê¹…ì„ ìœ„í•´:

```bash
# ë””ë²„ê·¸ ë¡œê·¸ + headless ë¹„í™œì„±í™” (ë¸Œë¼ìš°ì € ì°½ í‘œì‹œ)
python -m crawlers.worker --test coupang --url "..." --log-level DEBUG
```

### í ì´ˆê¸°í™”

ê°œë°œ ì¤‘ íë¥¼ ì´ˆê¸°í™”í•˜ë ¤ë©´:

```python
from storage.redis_client import task_queue

# ëª¨ë“  í ì´ˆê¸°í™”
task_queue.clear_queue("all")

# íŠ¹ì • íë§Œ ì´ˆê¸°í™”
task_queue.clear_queue("crawl")  # í¬ë¡¤ë§ í
task_queue.clear_queue("result")  # ê²°ê³¼ í
task_queue.clear_queue("dead_letter")  # ì‹¤íŒ¨ í
```

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:

1. **ë¡œê·¸ íŒŒì¼**: ì—ëŸ¬ ë©”ì‹œì§€ì™€ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ í™•ì¸
2. **í™˜ê²½ ë³€ìˆ˜**: ëª¨ë“  í•„ìˆ˜ ì„¤ì •ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
3. **ì˜ì¡´ì„±**: Redis, PostgreSQL, Chromeì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
4. **ë¦¬ì†ŒìŠ¤**: ë©”ëª¨ë¦¬, CPU, ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸

## ğŸ“ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” Price Sense ì „ìš© í¬ë¡¤ë§ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

---

**Price Sense í¬ë¡¤ë§ ì‹œìŠ¤í…œ** - í•œêµ­ ì´ì»¤ë¨¸ìŠ¤ ê°€ê²© ëª¨ë‹ˆí„°ë§ì˜ ìƒˆë¡œìš´ í‘œì¤€ ğŸš€